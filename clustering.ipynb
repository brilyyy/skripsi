{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case folding done\n"
     ]
    }
   ],
   "source": [
    "# case folding\n",
    "import json\n",
    "import re\n",
    "\n",
    "with open('data/documents-id.json') as abstracts_json:\n",
    "  abstracts = json.load(abstracts_json)\n",
    "\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  answer = re.sub('[^a-z]+', ' ', abstract['abstrak'].casefold())\n",
    "  abstracts[i]['abstrak'] = answer\n",
    "\n",
    "with open('data/case_folded.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n",
    "        \n",
    "print('Case folding done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "import json\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from mpstemmer import MPStemmer\n",
    "\n",
    "stemmer = MPStemmer()\n",
    "\n",
    "with open('data/case_folded.json') as case_folded_json:\n",
    "  abstracts = json.load(case_folded_json)\n",
    "\n",
    "# factory = StemmerFactory()\n",
    "# stemmer = factory.create_stemmer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  stemmed = stemmer.stem_kalimat(abstract['abstrak'])\n",
    "  abstracts[i]['abstrak'] = stemmed\n",
    "\n",
    "\n",
    "with open('data/stemmed_abstracts.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering\n",
    "import json\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "\n",
    "with open('data/stemmed_abstracts.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "additional_stopwords = []\n",
    "\n",
    "stopwords = StopWordRemoverFactory().get_stop_words() + additional_stopwords\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  words = abstract['abstrak'].split()\n",
    "  filtered = ''\n",
    "  for word in words:\n",
    "    if word not in stopwords:\n",
    "      filtered += word + ' '\n",
    "  abstracts[i]['abstrak'] = filtered\n",
    "\n",
    "with open('data/filtered.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/brilyyy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "with open('data/filtered.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  frequency = nltk.FreqDist(nltk.word_tokenize(abstract['abstrak']))\n",
    "  abstracts[i]['frequency'] = frequency\n",
    "\n",
    "with open('data/tokenized.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term frequency\n",
    "import json\n",
    "\n",
    "with open('data/tokenized.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  result = {}\n",
    "  bagOfWordsCount = len(abstract['abstrak'].split())\n",
    "  for word, count in abstract['frequency'].items():\n",
    "    if(count / float(bagOfWordsCount)):\n",
    "      result[word] = count / float(bagOfWordsCount)\n",
    "  abstracts[i]['frequency'] = result\n",
    "\n",
    "with open('data/termfrequency.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse document frequency\n",
    "import json\n",
    "import math\n",
    "\n",
    "\n",
    "threshold = 4\n",
    "\n",
    "with open('data/tokenized.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "abstract_length = len(abstracts)\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  idfDict = dict.fromkeys(abstract['frequency'].keys(), 0)\n",
    "  result = {}\n",
    "  for key in idfDict.keys():\n",
    "    for abstract in abstracts:\n",
    "      if key in abstract['frequency']:\n",
    "        idfDict[key] += 1\n",
    "  \n",
    "  for word, val in idfDict.items():\n",
    "    if val > threshold:\n",
    "      result[word] = math.log10(abstract_length / float(val) + 1)\n",
    "  abstracts[i]['frequency'] = result\n",
    "\n",
    "with open('data/idf.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "import json\n",
    "\n",
    "with open('data/idf.json') as idf_json:\n",
    "  idfs = json.load(idf_json)\n",
    "\n",
    "with open('data/termfrequency.json') as tf_json:\n",
    "  tfs = json.load(tf_json)\n",
    "\n",
    "\n",
    "\n",
    "for i, idf in enumerate(idfs):\n",
    "  tfidf = {}\n",
    "  maxtfidf = 0\n",
    "  for word, val in idf['frequency'].items():\n",
    "    if tfs[i]['frequency'].get(word) is not None:\n",
    "      tfidf[word] = val*float(tfs[i]['frequency'].get(word))\n",
    "      if maxtfidf < tfidf[word]:\n",
    "        maxtfidf = tfidf[word]\n",
    "        maxtfidfword = word\n",
    "  idfs[i]['frequency'] = tfidf\n",
    "  idfs[i]['maxtfidfword'] = maxtfidfword\n",
    "\n",
    "with open('data/tfidf.json', 'w') as json_file:\n",
    "  json_file.write(json.dumps(idfs, sort_keys=True, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary\n",
    "import json\n",
    "\n",
    "with open('data/tfidf.json') as json_file:\n",
    "  documents = json.load(json_file)\n",
    "\n",
    "\n",
    "words = []\n",
    "\n",
    "for i, document in enumerate(documents):\n",
    "  for word, val in document['frequency'].items():\n",
    "    if word not in words:\n",
    "      words.append(word)\n",
    "\n",
    "words_dict = dict(zip(range(len(words)), words))\n",
    "\n",
    "with open('data/dictionary.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(words_dict, sort_keys=True, indent=4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace word with dictionary id\n",
    "\n",
    "import json\n",
    "\n",
    "with open('data/dictionary.json') as json_file:\n",
    "  words = json.load(json_file)\n",
    "with open('data/tfidf.json') as json_file:\n",
    "  documents = json.load(json_file)\n",
    "\n",
    "# define function to get key from dictionary\n",
    "def get_key(val):\n",
    "  for key, value in words.items():\n",
    "    if val == value:\n",
    "      return key\n",
    "\n",
    "for i, document in enumerate(documents):\n",
    "  result = {}\n",
    "  for word, val in document['frequency'].items():\n",
    "      if word in words.values():\n",
    "        result[get_key(word)] = val\n",
    "  documents[i]['frequency'] = result\n",
    "\n",
    "with open('data/dictionarized.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(documents, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Clusters\n",
      "[[0.02344969 0.00410051 0.00479318 ... 0.         0.         0.        ]\n",
      " [0.02423509 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01481962 0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.02046518 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01114835 0.00268049 0.00313328 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brilyyy/Documents/GitHub/skripsi/venv/lib/python3.11/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hierarchical clustering\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.cluster.vq as scv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pandas import DataFrame\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "N_CLUSTERS = 3\n",
    "\n",
    "with open('data/dictionary.json') as json_file:\n",
    "  dictionary = json.load(json_file)\n",
    "\n",
    "with open('data/dictionarized.json') as json_file:\n",
    "  documents = json.load(json_file)\n",
    "\n",
    "# \n",
    "\n",
    "print('Calculating Clusters')\n",
    "array = []\n",
    "for i, document in enumerate(documents):\n",
    "\tnew = []\n",
    "\tfor key in dictionary.keys():\n",
    "\t\tif not document['frequency'].get(key) is None:\n",
    "\t\t\tnew.append(document['frequency'].get(key))\n",
    "\t\telse:\n",
    "\t\t\tnew.append(0)\n",
    "\tarray.append(new)\n",
    "\n",
    "X = np.array(array)\n",
    "print(X)\n",
    "titles = []\n",
    "\n",
    "for document in documents:\n",
    "\ttitles.append(document['judul'])\n",
    "\n",
    "hierachical = AgglomerativeClustering(n_clusters=N_CLUSTERS, affinity='euclidean', linkage='ward')\n",
    "cluster = hierachical.fit_predict(X)\n",
    "\n",
    "# kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0).fit(X)\n",
    "kmeans = KMedoids(n_clusters=N_CLUSTERS).fit(X)\n",
    "\n",
    "\n",
    "# for i in range(len(cluster)):\n",
    "# \tprint(f'Cluster: {cluster[i]}, Title: {titles[i]} \\n')\n",
    "\n",
    "# fig, dendogram = plt.subplots(figsize=(30,40))\n",
    "\n",
    "# dendogram = sch.dendrogram(sch.linkage(array, method='ward'), orientation='top')\n",
    "# plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plt.ylabel('Article')\n",
    "# plt.xlabel('Euclidean Distance')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('t'+str(4)+'sw'\n",
    "#             +'dendogram.png', dpi=200)\n",
    "\n",
    "dataframe = DataFrame({\n",
    "\t\t'Cluster': cluster,\n",
    "\t\t'Title': titles,\n",
    "})\n",
    "\n",
    "kmeans_dataframe = DataFrame({\n",
    "\t\t'Cluster': kmeans.labels_.tolist(),\n",
    "\t\t'Title': titles,\n",
    "})\n",
    "\n",
    "sorted_data_frame = dataframe.sort_values(by=['Cluster'])\n",
    "sorted_data_frame.to_excel('hierarchical_clustering.xlsx', index=False)\n",
    "sorted_data_frame = kmeans_dataframe.sort_values(by=['Cluster'])\n",
    "sorted_data_frame.to_excel('kmeans_clustering.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82d74610fba66d22e01faa28a65a289a119f27a0b2a61fc6029b1d06947e4a01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
