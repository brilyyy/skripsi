{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyquery\n",
    "#1 CRAWLING\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pyquery\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "data = []\n",
    "link_abstraks = []\n",
    "data_abstrak = []\n",
    "##ambil semua link abstrak di digilib\n",
    "print('PROSES AMBIL LINK DIGILIB')\n",
    "for x in range(1,376):\n",
    "\t# time.sleep(1)\n",
    "\tprint(\"page : https://digilib.uns.ac.id/dokumen/fakultas/3/Fak-MIPA/\"+str(x))\n",
    "\tresponse = requests.get('https://digilib.uns.ac.id/dokumen/fakultas/3/Fak-MIPA/'+str(x))\n",
    "\thomepage = BeautifulSoup(response.content, 'html.parser')\n",
    "\tlinks = homepage.find_all('a')\n",
    "\tfor link in links:\n",
    "\t\tif 'https://digilib.uns.ac.id/dokumen/detail' in link.get('href'):\n",
    "\t\t\tlink_abstraks += [link.get('href')]\n",
    "with open('datanim.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(link_abstraks, sort_keys=True, indent=4))\n",
    "\n",
    "##ambil nama, nim dan abstrak berdasarkan prodi informatika dengan data link yang didapat\n",
    "print(\"PROSES AMBIL DATA ABSTRAK\")\n",
    "i = 0\n",
    "count = len(link_abstraks)\n",
    "for link_abstrak in link_abstraks:\n",
    "\ttry:\n",
    "\t\tcount = count - 1\n",
    "\t\tresponse = requests.get(link_abstrak)\n",
    "\t\tpage = BeautifulSoup(response.content, 'html.parser')\n",
    "\t\tlinks = page.find_all('tr')\n",
    "\t\t##ambil nim\n",
    "\t\ttdnim = links[2].find_all('td')\n",
    "\t\tnim = tdnim[2].get_text()\n",
    "\t\tprint('remain'+str(count)+' '+nim)\n",
    "\n",
    "\t\t##Cek kalau prodi informatika bedasarkan nim M05\n",
    "\t\tif 'M05' in nim or 'M.05' in nim or 'M. 05' in nim or 'M 05' in nim:\n",
    "\t\t\tprint('dapat' + nim)\n",
    "\t\t\ti = i+1\n",
    "\t\t\tarray = {}\n",
    "\t\t\t##ambil abstrak\n",
    "\t\t\ttdabstrak = links[13].find_all('td')\n",
    "\t\t\tabstrak = tdabstrak[2].get_text()\n",
    "\t\t\t# array['abstrak'] = abstrak\n",
    "\t\t\t##ambil judul\n",
    "\t\t\ttdjudul = links[4].find_all('td')\n",
    "\t\t\tjudul = tdjudul[2].get_text()\n",
    "\t\t\t# array['list'] = judul\n",
    "\t\t\tdata_abstrak += [[judul,abstrak,str(i-1)]]\n",
    "\t\t\twith open('dataabstrak.json', 'w') as outfile:\n",
    "\t\t\t\toutfile.write(json.dumps(data_abstrak, sort_keys=True, indent=4))\n",
    "\texcept Exception as e: \n",
    "\t\tprint(e)\n",
    "print(\"SELESAI\")\n",
    "print(\"JUMLAH DATA ABSTRAK YANG DI DAPAT :\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 CASE FOLDING\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "processed=[]\n",
    "count=0\n",
    "with open('dataabstrak.json') as json_file:\n",
    "\tarticles = json.load(json_file)\n",
    "for  article in articles:\n",
    "\t# folded=article[1].casefold()/\n",
    "\tanswer = re.sub('[^a-z]+', ' ', article[1].casefold())\n",
    "\tarticles[count][1] = answer\n",
    "\tcount+=1\n",
    "\t# Progress Report\n",
    "\tprint(str(count)+'/'+str(len(articles)))\n",
    "\t# sys.stdout.write(\"\\033[F\")\n",
    "with open('casefolded.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(articles, sort_keys=True, indent=4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install sastrawi\n",
    "##3 STEMMING\n",
    "import json\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "with open('casefolded.json') as json_file:\n",
    "\tarticles = json.load(json_file)\n",
    "count = 0\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "for article in articles:\n",
    "\tstemmed =stemmer.stem(article[1]) \n",
    "\tprint (str(count)+\"/\"+str(len(articles)))\n",
    "\tarticles[count][1] = stemmed\n",
    "\tstemmed =stemmer.stem(article[2]) \n",
    "\tprint (str(count)+\"/\"+str(len(articles)))\n",
    "\tarticles[count][2] = stemmed\n",
    "\tcount += 1\n",
    "with open('stemmed.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(articles, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##4 FILTERING\n",
    "threshold = 4\n",
    "clustercount = 16\n",
    "\n",
    "\n",
    "from os import system, name \n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = \"\\r\"):\n",
    " \tpercent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    " \tfilledLength = int(length * iteration // total)\n",
    " \tbar = fill * filledLength + '-' * (length - filledLength)\n",
    " \tprint(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    " \tif iteration == total:\n",
    " \t\tprint()\n",
    " \t\t\n",
    "\n",
    "\n",
    "import json\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "\n",
    "\n",
    "with open('stemmed.json') as json_file:\n",
    "\tarticles = json.load(json_file)\n",
    "print('filtering')\n",
    "count = 0\n",
    "additionalstopwords=[]\n",
    "# additionalstopwords+=[\"satu\",\"dua\",\"tiga\",\"empat\",\"lima\",\"enam\",\"tujuh\",\"delapan\",\"sembilan\",\"sepuluh\",\"belas\",\"puluh\",\"ratus\",\"ribu\",\"juta\"]\n",
    "# additionalstopwords+=[\"januari\",\"februari\",\"maret\",\"april\",\"mei\",\"juni\",\"juli\",\"agustus\",\"september\",\"oktober\",\"november\",\"desember\"]\n",
    "stopwords = StopWordRemoverFactory().get_stop_words()+additionalstopwords\n",
    "l = len(articles)\n",
    "printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "for article in articles:\n",
    "\twords = article[1].split()\n",
    "\tfiltered = \"\"\n",
    "\tfor word in words:\n",
    "\t\tif word not in stopwords:\n",
    "\t\t\tfiltered += word+\" \" \n",
    "\tarticles[count][1] = filtered\n",
    "\tcount += 1\n",
    "\tprintProgressBar(count, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk\n",
    "##5 TOKENIZING\n",
    "\n",
    "with open('filtered.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(articles, sort_keys=True, indent=4))\n",
    "try:\n",
    "\timport json\n",
    "\timport re\n",
    "\timport time\n",
    "\timport sys\n",
    "\timport nltk\n",
    "except ImportError:\n",
    "    pass\n",
    "nltk.download('punkt')\n",
    "print('tokenizing')\n",
    "count=0\n",
    "with open('filtered.json') as json_file:\n",
    "\tarticles = json.load(json_file)\n",
    "printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "for article in articles:\n",
    "\tfrequency = nltk.FreqDist(nltk.word_tokenize(article[1]))\n",
    "\tarticles[count][1] = frequency\n",
    "\tcount+=1\n",
    "\tprintProgressBar(count, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "\t# print(str(count)+'/'+str(len(articles)))\n",
    "\t# frequencies=nltk.FreqDist(processed)\n",
    "\t\n",
    "# print(frequencies.most_common())\n",
    "# frequencies.plot(30,cumulative=False)\n",
    "# print(processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##6 TERMFREQUENCY\n",
    "with open('tokenized.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(articles, sort_keys=True, indent=4))\n",
    "try:\n",
    "\timport json\n",
    "\timport nltk\n",
    "except ImportError:\n",
    "    pass\n",
    "print('Calculating Term Frequency')\n",
    "count=0\n",
    "counter=0\n",
    "results=[]\n",
    "with open('tokenized.json') as json_file:\n",
    "\tarticles = json.load(json_file)\n",
    "with open('filtered.json') as json_file:\n",
    "\tfiltered = json.load(json_file)\n",
    "printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "for article in articles:\n",
    "\tresult = {}\n",
    "\t# print(str(counter)+\" / \"+str(len(articles)))\n",
    "\tbagOfWordsCount = len(filtered[counter][1])\n",
    "\tfor word, count in article[1].items():\n",
    "\t\tif((count/float(bagOfWordsCount))):\n",
    "\t\t\tresult[word] = count / float(bagOfWordsCount)\n",
    "\t\t# print(word+str(result[word]))\n",
    "\tarticles[counter][1]=result\n",
    "\tcounter+=1\n",
    "\tprintProgressBar(counter, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##7 INVERSE DOCUMENT FREQUENCY\n",
    "with open('termfrequency.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(articles, sort_keys=True, indent=4))\t\n",
    "\n",
    "try:\n",
    "\timport json\n",
    "\timport nltk\n",
    "\timport math\n",
    "except ImportError:\n",
    "    pass\n",
    "count=0\n",
    "counter=0\n",
    "with open('tokenized.json') as json_file:\n",
    "\tarticles = json.load(json_file)\n",
    "print('Calculating Inverse Document Frequency')\n",
    "N = len(articles)\n",
    "printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "for article in articles:\n",
    "\tidfDict = dict.fromkeys(article[1].keys(), 0)\n",
    "\tresult = {}\n",
    "\tfor key in idfDict.keys():\n",
    "\t\tfor article in articles:\n",
    "\t\t\tif key in article[1]:\n",
    "\t\t\t\tidfDict[key] += 1\n",
    "\tfor word, val in idfDict.items():\n",
    "\t\t# if 50>val>4:\n",
    "\t\tif val>threshold:\n",
    "\t\t\tresult[word] = math.log10(N / (float(val)+1))\n",
    "\t\t# else:\n",
    "\t\t# \tprint(word+\" omitted, df = \"+str(val))\n",
    "\n",
    "\tarticles[count][1] = result\n",
    "\tcount+=1\n",
    "\tprintProgressBar(count, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##8 TFIDF\n",
    "with open('idfed.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(articles, sort_keys=True, indent=4))\t\n",
    "\n",
    "\n",
    "try:\n",
    "\timport json\n",
    "except ImportError:\n",
    "    pass\n",
    "print('Calculating TF-IDF')\n",
    "count=0\n",
    "counter=0\n",
    "tfidf = []\n",
    "with open('idfed.json') as json_file:\n",
    "\tidfs = json.load(json_file)\n",
    "with open('termfrequency.json') as json_file:\n",
    "\ttfs = json.load(json_file)\n",
    "printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "for idf in idfs:\n",
    "\ttfidf={}\n",
    "\ttfs_dict = tfs[count][1].items()\n",
    "\tmaxtfidf = 0\n",
    "\tfor word, val in idf[1].items():\n",
    "\t\t# if val*tfs[count][1].get(word)>0:\n",
    "\t\t\t# print(val*tfs[count][1].get(word))\n",
    "\t\tif tfs[count][1].get(word) is not None:\n",
    "\t\t\ttfidf[word] = val*float(tfs[count][1].get(word))\n",
    "\t\t\tif maxtfidf<tfidf[word]:\n",
    "\t\t\t\tmaxtfidf=tfidf[word]\n",
    "\t\t\t\tmaxtfidfword = word\n",
    "\tidfs[count][1]=tfidf\n",
    "\tidfs[count] += [maxtfidfword]\n",
    "\n",
    "\tcount+=1\n",
    "\tprintProgressBar(count, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 DICTIONARY\n",
    "with open('tfidfed.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(idfs, sort_keys=True, indent=4))\t\n",
    "try:\n",
    "\timport json\n",
    "except ImportError:\n",
    "    pass\n",
    "print('Building dictionary')\n",
    "\n",
    "count1=0\n",
    "\n",
    "i=0\n",
    "tfidf = []\n",
    "words={}\n",
    "result=[]\n",
    "with open('tfidfed.json') as json_file:\n",
    "\tdocuments = json.load(json_file)\n",
    "# print (documents[0][1])\n",
    "# print (documents[0].pop(1))\n",
    "for document in documents:\n",
    "\tcount2=0\n",
    "\tcek={}\n",
    "\tfor word in document[1].items():\n",
    "\t\t# result.append(count1)\n",
    "\t\tif word[0] in words.values():\n",
    "\t\t\t# print(\"match\")\n",
    "\t\t\tfor key,value in words.items():\n",
    "\t\t\t\tif word[0] == value:\n",
    "\t\t\t\t\t# print(\"match \"+word[0])\n",
    "\t\t\t\t\tcek[key]=word[1]\n",
    "\t\t\t\t\t# print(word[1])\n",
    "\t\telse:\n",
    "\t\t\t# print(\"added \"+str(i)+\" = \"+word[0])\n",
    "\t\t\twords[i]=word[0]\n",
    "\t\t\t# print(words[i]+\" = \"+str(word[0]))\n",
    "\t\t\tcek[i]=word[1]\n",
    "\t\t\ti+=1;\n",
    "\t\t\t# print('added '+word)\n",
    "\t\tcount2+=1\n",
    "\n",
    "\t# print(cek)\n",
    "\n",
    "\tresult=[cek]\n",
    "\t# documents[count1].pop(1)\n",
    "\tdocuments[count1][1]=cek\n",
    "\n",
    "\tcount1+=1\n",
    "\n",
    "# print(words)\n",
    "# print(words)\n",
    "\n",
    "# print(count1)\n",
    "\n",
    "with open('dictionary.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(words, sort_keys=True, indent=4))\t\n",
    "\n",
    "with open('dictionarized.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(documents, sort_keys=True, indent=4))\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 hierarchical clustering\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.cluster.vq as scv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pandas import DataFrame\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "with open('dictionarized.json') as json_file:\n",
    "\tdocuments = json.load(json_file)\n",
    "with open('dictionary.json') as json_file:\n",
    "\tdictionary = json.load(json_file)\n",
    "count=0\n",
    "\n",
    "# for document in documents:\n",
    "# \tdocuments[count][0]=count\n",
    "# \tcount+=1\n",
    "# X = [[i] for i in documents]\n",
    "# print(X)\n",
    "print('Calculating Clusters')\n",
    "array=[]\n",
    "\n",
    "for document in documents:\n",
    "\tcount2=0\n",
    "\tarray.append([])\n",
    "\tfor key in dictionary.keys():\n",
    "\t\t\n",
    "\t\tif not document[1].get(key) is None:\n",
    "\t\t\tarray[count].append(document[1].get(key))\n",
    "\t\telse:\n",
    "\t\t\tarray[count].append(0)\n",
    "\tcount+=1\n",
    "\t# if count == 10:\n",
    "\t# \tbreak;\n",
    "X=np.array(array)\n",
    "counter=0\n",
    "titles=[]\n",
    "kanal = []\n",
    "subkanal = []\n",
    "topword = []\n",
    "for n in documents:\n",
    "\ttitles+=[n[0]]\n",
    "\t# kanal+=[n[2]]\n",
    "\t# subkanal+=[n[3]]\n",
    "\t# topword+=[n[4]]\n",
    "\tcounter+=1\n",
    "\n",
    "\n",
    "# print(X)\n",
    "# X.set_index(titles)\n",
    "# print(X)\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=clustercount, affinity='euclidean', linkage='ward')\n",
    "cluster = clustering.fit_predict(X)\n",
    "print(clustering.labels_)\n",
    "# plt.scatter(X[:,0],X[:,1], c=cluster.labels_, cmap='rainbow')\n",
    "# plt.show()\n",
    "\n",
    "clf = NearestCentroid()\n",
    "clf.fit(X, cluster)\n",
    "centroids = clf.centroids_\n",
    "# print(array[200])\n",
    "fig, dendogram = plt.subplots(figsize=(30, 40))\n",
    "dendrogram = sch.dendrogram(sch.linkage(array, method  = \"ward\"),orientation = \"right\")\n",
    "plt.title('Dendrogram')\n",
    "plt.ylabel('Article')\n",
    "plt.xlabel('Euclidean distances')\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig('t'+str(threshold)+'sw'+str(len(additionalstopwords))+'dendogram.png', dpi=200)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = DataFrame(array,columns=dictionary.values())\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11 K-MEANS\n",
    "kmeans = KMeans(n_clusters=clustercount, init=centroids, n_init=1, max_iter=1000).fit(df)\n",
    "centroids = kmeans.cluster_centers_\n",
    "print(kmeans.labels_)\n",
    "# print(df)\n",
    "clusters=kmeans.labels_.tolist()\n",
    "# print(clusters)\n",
    "print(\"threshold = \"+str(threshold))\n",
    "print(\"thresholdmax = \"+str(thresholdmax))\n",
    "print(\"stopwords = \"+str(additionalstopwords))\n",
    "for n in range(clustercount):\n",
    "\tprint(\"cluster \"+str(n)+\" = \"+str(clusters.count(n)))\n",
    "dataframe = DataFrame({\n",
    "\t\"count\":range(len(titles)),\n",
    "\t\"cluster\":clusters,\n",
    "\t\"title\":titles,\n",
    "\t# \"kanal\":kanal,\n",
    "\t# \"subkanal\":subkanal,\n",
    "\t# \"top word\":topword\n",
    "\n",
    "\t})\n",
    "sortedres=dataframe.sort_values('cluster')\n",
    "sortedres.to_csv('t'+str(threshold)+'sw'+str(len(additionalstopwords))+'result.csv')\n",
    "# with open('result.json','w') as outfile:\n",
    "\t# outfile.write(json.dumps(dataframe, sort_keys=True, indent=4))\t\n",
    "with open('t'+str(threshold)+'sw'+str(len(additionalstopwords))+'summary.txt', 'w') as f:\n",
    "\tprint(\"threshold = \"+str(threshold), file=f)\n",
    "\tprint(\"stopwords = \"+str(additionalstopwords), file=f)\n",
    "\tfor n in range(10):\n",
    "\t\tprint(\"cluster \"+str(n)+\" = \"+str(clusters.count(n)), file=f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('filtered.json') as json_file:\n",
    "\tdocuments = json.load(json_file)\n",
    "clusterwords = [\"\"]*clustercount\n",
    "count = 0\n",
    "for cluster in clusters:\n",
    "\tclusterwords[cluster]+=documents[count][1]\n",
    "\tcount+=1\n",
    "count = 0\n",
    "for clusterword in clusterwords:\n",
    "\tfrequency = nltk.FreqDist(nltk.word_tokenize(clusterword))\n",
    "\tclusterwords[count] = frequency\n",
    "\tcount+=1\n",
    "\t# frequencies=nl\n",
    "\n",
    "from collections import Counter\n",
    "count = 0\n",
    "for clusterword in clusterwords:\n",
    "\t\n",
    "\tclusterwords[count]=dict(Counter(clusterword).most_common(20))\n",
    "\tcount+=1\n",
    "with open('t'+str(threshold)+'sw'+str(len(additionalstopwords))+'clusterword.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(clusterwords, sort_keys=True, indent=4))\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "95334295d287576637839bb6c89b1e9ed45a51d991237be8e658a0ed30ccbece"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
