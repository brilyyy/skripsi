{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "\n",
    "\n",
    "def progress_bar():\n",
    "    return progressbar.ProgressBar(maxval=78, widgets=[\n",
    "        ' [', progressbar.Timer(), '] ',\n",
    "        progressbar.Bar(marker='0', left='[', right=']'),\n",
    "        ' (', progressbar.ETA(), ') ',\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting link from page 1Getting link from page 2\n",
      "\n",
      "Getting link from page 3\n",
      "Getting link from page 4\n",
      "Getting link from page 5\n",
      "Getting link from page 6\n",
      "Getting link from page 7\n",
      "Getting link from page 8\n",
      "Getting link from page 9\n",
      "Getting link from page 10\n",
      "Getting link from page 11\n",
      "Getting link from page 12\n",
      "Getting link from page 13\n",
      "Getting link from page 14\n",
      "Getting link from page 15\n",
      "Getting link from page 16\n",
      "Getting link from page 17\n",
      "Getting link from page 18\n",
      "Getting link from page 19\n",
      "Getting link from page 20\n",
      "Getting link from page 21\n",
      "Getting link from page 22\n",
      "Getting link from page 23\n",
      "Getting link from page 24\n",
      "Getting link from page 25\n",
      "Getting link from page 26\n",
      "Getting link from page 27\n",
      "Getting link from page 28\n",
      "Getting link from page 29\n",
      "Getting link from page 30\n",
      "Getting link from page 31\n",
      "Getting link from page 32\n",
      "Getting link from page 33\n",
      "Getting link from page 34\n",
      "Getting link from page 35\n",
      "Getting link from page 36\n",
      "Getting link from page 37\n",
      "Getting link from page 38\n",
      "Getting link from page 39\n",
      "Getting link from page 40\n",
      "Getting link from page 41\n",
      "Getting link from page 42\n",
      "Getting link from page 43\n",
      "Getting link from page 44\n",
      "Getting link from page 45\n",
      "Getting link from page 46\n",
      "Getting link from page 47\n",
      "Getting link from page 48\n",
      "Getting link from page 49\n",
      "Getting link from page 50\n",
      "Getting link from page 51\n",
      "Getting link from page 52\n",
      "Getting link from page 53\n",
      "Getting link from page 54\n",
      "Getting link from page 55\n",
      "Getting link from page 56\n",
      "Getting link from page 57\n",
      "Getting link from page 58\n",
      "Getting link from page 59\n",
      "Getting link from page 60\n",
      "Getting link from page 61\n",
      "Getting link from page 62\n",
      "Getting link from page 63\n",
      "Getting link from page 64\n",
      "Getting link from page 65\n",
      "Getting link from page 66\n",
      "Getting link from page 67\n",
      "Getting link from page 68\n",
      "Getting link from page 69\n",
      "Getting link from page 70\n",
      "Getting link from page 71\n",
      "Getting link from page 72\n",
      "Getting link from page 73\n",
      "Getting link from page 74Getting link from page 75\n",
      "Getting link from page 76\n",
      "\n",
      "Getting link from page 77\n",
      "Getting link from page 78\n",
      "Getting link from page 79\n",
      "Getting link from page 80\n",
      "Getting link from page 81\n",
      "Getting link from page 82\n",
      "Getting link from page 83\n",
      "Getting link from page 84\n",
      "Getting link from page 85\n",
      "Getting link from page 86\n",
      "Getting link from page 87\n",
      "Getting link from page 88\n",
      "Getting link from page 89\n",
      "Getting link from page 90\n",
      "Getting link from page 91\n",
      "Getting link from page 92\n",
      "Getting link from page 93\n",
      "Getting link from page 94\n",
      "Getting link from page 95\n",
      "Getting link from page 96\n",
      "Getting link from page 97\n",
      "Getting link from page 98\n",
      "Getting link from page 99\n",
      "Getting link from page 100\n",
      "Getting link from page 101\n",
      "Getting link from page 102Getting link from page 103\n",
      "\n",
      "Getting link from page 104\n",
      "Getting link from page 105\n",
      "Getting link from page 106\n",
      "Getting link from page 107\n",
      "Getting link from page 108\n",
      "Getting link from page 109\n",
      "Getting link from page 110\n",
      "Getting link from page 111\n",
      "Getting link from page 112\n",
      "Getting link from page 113\n",
      "Getting link from page 114\n",
      "Getting link from page 115\n",
      "Getting link from page 116\n",
      "Getting link from page 117\n",
      "Getting link from page 118\n",
      "Getting link from page 119\n",
      "Getting link from page 120\n",
      "Getting link from page 121\n",
      "Getting link from page 122\n",
      "Getting link from page 123\n",
      "Getting link from page 124\n",
      "Getting link from page 125\n",
      "Getting link from page 126\n",
      "Getting link from page 127\n",
      "Getting link from page 128Getting link from page 129\n",
      "\n",
      "Getting link from page 130\n",
      "Getting link from page 131\n",
      "Getting link from page 132\n",
      "Getting link from page 133\n",
      "Getting link from page 134\n",
      "Getting link from page 135\n",
      "Getting link from page 136\n",
      "Getting link from page 137\n",
      "Getting link from page 138\n",
      "Getting link from page 139\n",
      "Getting link from page 140\n",
      "Getting link from page 141\n",
      "Getting link from page 142\n",
      "Getting link from page 143\n",
      "Getting link from page 144\n",
      "Getting link from page 145\n",
      "Getting link from page 146\n",
      "Getting link from page 147\n",
      "Getting link from page 148\n",
      "Getting link from page 149\n",
      "Getting link from page 150\n",
      "Getting link from page 151\n",
      "Getting link from page 152\n",
      "Getting link from page 153\n",
      "Getting link from page 154\n",
      "Getting link from page 155\n",
      "Getting link from page 156\n",
      "Getting link from page 157\n",
      "Getting link from page 158\n",
      "Getting link from page 159\n",
      "Getting link from page 160\n",
      "Getting link from page 161\n",
      "Getting link from page 162\n",
      "Getting link from page 163\n",
      "Getting link from page 164\n",
      "Getting link from page 165\n",
      "Getting link from page 166\n",
      "Getting link from page 167\n",
      "Getting link from page 168\n",
      "Getting link from page 169\n",
      "Getting link from page 170\n",
      "Getting link from page 171\n",
      "Getting link from page 172\n",
      "Getting link from page 173\n",
      "Getting link from page 174\n",
      "Getting link from page 175\n",
      "Getting link from page 176\n",
      "Getting link from page 177\n",
      "Getting link from page 178\n",
      "Getting link from page 179\n",
      "Getting link from page 180\n",
      "Getting link from page 181\n",
      "Getting link from page 182\n",
      "Getting link from page 183\n",
      "Getting link from page 184\n",
      "Getting link from page 185\n",
      "Getting link from page 186\n",
      "Getting link from page 187\n",
      "Getting link from page 188\n",
      "Getting link from page 189\n",
      "Getting link from page 190\n",
      "Getting link from page 191\n",
      "Getting link from page 192\n",
      "Getting link from page 193\n",
      "Getting link from page 194\n",
      "Getting link from page 195\n",
      "Getting link from page 196\n",
      "Getting link from page 197\n",
      "Getting link from page 198\n",
      "Getting link from page 199\n",
      "Getting link from page 200\n",
      "Getting link from page 201\n",
      "Getting link from page 202\n",
      "Getting link from page 203\n",
      "Getting link from page 204\n",
      "Getting link from page 205\n",
      "Getting link from page 206\n",
      "Getting link from page 207\n",
      "Getting link from page 208\n",
      "Getting link from page 209\n",
      "Getting link from page 210Getting link from page 211\n",
      "\n",
      "Getting link from page 212\n",
      "Getting link from page 213\n",
      "Getting link from page 214Getting link from page 215\n",
      "\n",
      "Getting link from page 216\n",
      "Getting link from page 217\n",
      "Getting link from page 218\n",
      "Getting link from page 219\n",
      "Getting link from page 220\n",
      "Getting link from page 221\n",
      "Getting link from page 222\n",
      "Getting link from page 223\n",
      "Getting link from page 224\n",
      "Getting link from page 225\n",
      "Getting link from page 226\n",
      "Getting link from page 227\n",
      "Getting link from page 228Getting link from page 229\n",
      "\n",
      "Getting link from page 230\n",
      "Getting link from page 231\n",
      "Getting link from page 232\n",
      "Getting link from page 233\n",
      "Getting link from page 234\n",
      "Getting link from page 235\n",
      "Getting link from page 236\n",
      "Getting link from page 237\n",
      "Getting link from page 238\n",
      "Getting link from page 239\n",
      "Getting link from page 240\n",
      "Getting link from page 241\n",
      "Getting link from page 242\n",
      "Getting link from page 243\n",
      "Getting link from page 244\n",
      "Getting link from page 245\n",
      "Getting link from page 246\n",
      "Getting link from page 247\n",
      "Getting link from page 248\n",
      "Getting link from page 249\n",
      "Getting link from page 250\n",
      "Getting link from page 251\n",
      "Getting link from page 252\n",
      "Getting link from page 253\n",
      "Getting link from page 254\n",
      "Getting link from page 255\n",
      "Getting link from page 256\n",
      "Getting link from page 257\n",
      "Getting link from page 258\n",
      "Getting link from page 259\n",
      "Getting link from page 260\n",
      "Getting link from page 261\n",
      "Getting link from page 262\n",
      "Getting link from page 263\n",
      "Getting link from page 264\n",
      "Getting link from page 265\n",
      "Getting link from page 266\n",
      "Getting link from page 267\n",
      "Getting link from page 268Getting link from page 269\n",
      "\n",
      "Getting link from page 270Getting link from page 271\n",
      "Getting link from page 272\n",
      "\n",
      "Getting link from page 273Getting link from page 274\n",
      "\n",
      "Getting link from page 275\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/brilyyy/Documents/Git/skripsi/clustering.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brilyyy/Documents/Git/skripsi/clustering.ipynb#ch0000001?line=31'>32</a>\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdetail_links.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m output:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brilyyy/Documents/Git/skripsi/clustering.ipynb#ch0000001?line=32'>33</a>\u001b[0m     output\u001b[39m.\u001b[39mwrite(json\u001b[39m.\u001b[39mdumps(links, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, sort_keys\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brilyyy/Documents/Git/skripsi/clustering.ipynb#ch0000001?line=34'>35</a>\u001b[0m get_detail_links()\n",
      "\u001b[1;32m/Users/brilyyy/Documents/Git/skripsi/clustering.ipynb Cell 3\u001b[0m in \u001b[0;36mget_detail_links\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brilyyy/Documents/Git/skripsi/clustering.ipynb#ch0000001?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_detail_links\u001b[39m():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brilyyy/Documents/Git/skripsi/clustering.ipynb#ch0000001?line=29'>30</a>\u001b[0m   \u001b[39mwith\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mThreadPoolExecutor(max_workers\u001b[39m=\u001b[39mMAX_THREADS) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/brilyyy/Documents/Git/skripsi/clustering.ipynb#ch0000001?line=30'>31</a>\u001b[0m       executor\u001b[39m.\u001b[39mmap(get_link, \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, pages \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brilyyy/Documents/Git/skripsi/clustering.ipynb#ch0000001?line=31'>32</a>\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdetail_links.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m output:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/brilyyy/Documents/Git/skripsi/clustering.ipynb#ch0000001?line=32'>33</a>\u001b[0m     output\u001b[39m.\u001b[39mwrite(json\u001b[39m.\u001b[39mdumps(links, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, sort_keys\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:637\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 637\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshutdown(wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    638\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         t\u001b[39m.\u001b[39;49mjoin()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:1060\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1061\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1062\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/threading.py:1080\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1080\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1081\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1082\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get all links to the detail page from digilib.uns.ac.id\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import json\n",
    "import concurrent.futures\n",
    "\n",
    "links = []\n",
    "\n",
    "abstracts_data = []\n",
    "\n",
    "pages = 1812\n",
    "\n",
    "MAX_THREADS = 8\n",
    "def get_link(page):\n",
    "  time.sleep(1)\n",
    "  print(f'Getting link from page {page}')\n",
    "  res = requests.get(f'https://digilib.uns.ac.id/dokumen/fakultas/7/Fak-KIP/{page}')\n",
    "  html_page = bs(res.content, 'html.parser')\n",
    "  document_cards = html_page.select(\n",
    "      '#digilib-body > div > div > div.col-md-8 > div.mb-5')\n",
    "  for card in document_cards:\n",
    "    document_type = card.select(\n",
    "        '.dokumen-search-body .detail div:nth-child(2)')[0].text.strip().lower()\n",
    "    if(document_type == 'skripsi'):\n",
    "      anchor = card.find('a')\n",
    "      if 'https://digilib.uns.ac.id/dokumen/detail/' in anchor.get('href'):\n",
    "          links.append(anchor['href'])\n",
    "\n",
    "def get_detail_links():\n",
    "  with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "      executor.map(get_link, range(1, pages + 1))\n",
    "  with open('detail_links.json', 'w') as output:\n",
    "    output.write(json.dumps(links, indent=4, sort_keys=True))\n",
    "\n",
    "get_detail_links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get abstract and title from detail page\n",
    "# def get_abstract_and_title(link):\n",
    "  \n",
    "#   res = requests.get(link)\n",
    "#   html_page = bs(res.content, 'html.parser')\n",
    "#   table = html_page.find('table', {'class': 'table table-responsive'})\n",
    "#   table_trs = table.find_all('tr')\n",
    "#   nim = table_trs[2].text.strip()\n",
    "#   if 'K35' in nim or 'K.35' in nim or 'K. 35' in nim or 'K 35' in nim:\n",
    "#     print('==============================')\n",
    "#     print(\n",
    "#         f'Getting abstract from link: {link} \\n')\n",
    "#     tdabstrak = table_trs[13].find_all('td')\n",
    "#     tdjudul = table_trs[4].find_all('td')\n",
    "#     abstract = tdabstrak[2].get_text()\n",
    "#     title = tdjudul[2].get_text()\n",
    "\n",
    "#     abstracts_data.append({\n",
    "#         'title': title,\n",
    "#         'abstract': abstract,\n",
    "#     })\n",
    "#   else:\n",
    "#     print('==============================')\n",
    "#     print(\n",
    "#         f'Link skipped: {link} \\n')\n",
    "#     time.sleep(1)\n",
    "\n",
    "\n",
    "# def add_id_to_abstract(i):\n",
    "#   abstracts_data[i]['id'] = i + 1\n",
    "\n",
    "\n",
    "# def get_abstracts():\n",
    "#   with open('detail_links.json', 'r') as input:\n",
    "#     links_from_json = json.load(input)\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "#         executor.map(get_abstract_and_title, (link for link in links_from_json))\n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "#         executor.map(add_id_to_abstract,\n",
    "#                      (i for i in range(len(abstracts_data))))\n",
    "#     with open('abstracts_data.json', 'w') as output:\n",
    "#       output.write(json.dumps(abstracts_data, indent=4, sort_keys=True))\n",
    "\n",
    "\n",
    "# get_abstracts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case folding\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import concurrent.futures\n",
    "\n",
    "with open('data/abstracts_data.json') as abstracts_json:\n",
    "  abstracts = json.load(abstracts_json)\n",
    "\n",
    "bar = progress_bar()\n",
    "bar.start()\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  bar.update(i + 1)\n",
    "  answer = re.sub('[^a-z]+', ' ', abstract['abstract'].casefold())\n",
    "  abstracts[i]['abstract'] = answer\n",
    "\n",
    "with open('data/case_folded.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n",
    "  \n",
    "bar.finish()\n",
    "print('Case folding done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate if english\n",
    "import json\n",
    "from langdetect import detect\n",
    "import translators as ts\n",
    "\n",
    "with open('data/case_folded.json') as case_folded_json:\n",
    "  abstracts = json.load(case_folded_json)\n",
    "\n",
    "bar = progress_bar().start()\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  bar.update(i + 1)\n",
    "  detector = detect(abstract['abstract'])\n",
    "  if detector == 'en':\n",
    "    translation = ts.google(abstract['abstract'], from_language='en', to_language='id')\n",
    "    abstracts[i]['abstract'] = translation\n",
    "\n",
    "\n",
    "with open('data/translated.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case folding again\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "processed = []\n",
    "count = 0\n",
    "with open('data/translated.json') as abstracts_json:\n",
    "  abstracts = json.load(abstracts_json)\n",
    "\n",
    "bar = progress_bar().start()\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  bar.update(i + 1)\n",
    "  answer = re.sub('[^a-z]+', ' ', abstract['abstract'].casefold())\n",
    "  abstracts[i]['abstract'] = answer\n",
    "\n",
    "with open('data/case_folded.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n",
    "\n",
    "bar.finish()\n",
    "print('Case folding done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "import json\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "with open('data/case_folded.json') as case_folded_json:\n",
    "  abstracts = json.load(case_folded_json)\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "\n",
    "bar = progress_bar()\n",
    "bar.start()\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  bar.update(i + 1)\n",
    "  stemmed = stemmer.stem(abstract['abstract'])\n",
    "  abstracts[i]['abstract'] = stemmed\n",
    "\n",
    "\n",
    "with open('data/stemmed_abstracts.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n",
    "\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering\n",
    "import json\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "\n",
    "with open('data/stemmed_abstracts.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "bar = progress_bar().start()\n",
    "\n",
    "additional_stopwords = []\n",
    "\n",
    "stopwords = StopWordRemoverFactory().get_stop_words() + additional_stopwords\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  bar.update(i + 1)\n",
    "  words = abstract['abstract'].split()\n",
    "  filtered = ''\n",
    "  for word in words:\n",
    "    if word not in stopwords:\n",
    "      filtered += word + ' '\n",
    "  abstracts[i]['abstract'] = filtered\n",
    "\n",
    "with open('data/filtered.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "with open('data/filtered.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "bar = progress_bar().start()\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  frequency = nltk.FreqDist(nltk.word_tokenize(abstract['abstract']))\n",
    "  abstracts[i]['frequency'] = frequency\n",
    "  bar.update(i + 1)\n",
    "\n",
    "with open('data/tokenized.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term frequency\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "with open('data/tokenized.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "bar = progress_bar().start()\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  result = {}\n",
    "  bagOfWordsCount = len(abstract['abstract'])\n",
    "  for word, count in abstract['frequency'].items():\n",
    "    if(count / float(bagOfWordsCount)):\n",
    "      result[word] = count / float(bagOfWordsCount)\n",
    "  abstracts[i]['frequency'] = result\n",
    "  bar.update(i + 1)\n",
    "\n",
    "with open('data/termfrequency.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse document frequency\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "\n",
    "threshold = 4\n",
    "\n",
    "with open('data/tokenized.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "bar = progress_bar().start()\n",
    "\n",
    "abstract_length = len(abstracts)\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  idfDict = dict.fromkeys(abstract['frequency'].keys(), 0)\n",
    "  result = {}\n",
    "  for key in idfDict.keys():\n",
    "    for abstract in abstracts:\n",
    "      if key in abstract['frequency']:\n",
    "        idfDict[key] += 1\n",
    "  \n",
    "  for word, val in idfDict.items():\n",
    "    if val > threshold:\n",
    "      result[word] = math.log10(abstract_length / float(val) + 1)\n",
    "  abstracts[i]['frequency'] = result\n",
    "  bar.update(i + 1)\n",
    "\n",
    "with open('data/idf.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "import json\n",
    "\n",
    "with open('data/idf.json') as idf_json:\n",
    "  idfs = json.load(idf_json)\n",
    "\n",
    "with open('data/termfrequency.json') as tf_json:\n",
    "  tfs = json.load(tf_json)\n",
    "\n",
    "bar = progress_bar().start()\n",
    "\n",
    "for i, idf in enumerate(idfs):\n",
    "  tfidf = {}\n",
    "  maxtfidf = 0\n",
    "  for word, val in idf['frequency'].items():\n",
    "    if tfs[i]['frequency'].get(word) is not None:\n",
    "      tfidf[word] = val*float(tfs[i]['frequency'].get(word))\n",
    "      if maxtfidf < tfidf[word]:\n",
    "        maxtfidf = tfidf[word]\n",
    "        maxtfidfword = word\n",
    "  idfs[i]['frequency'] = tfidf\n",
    "  idfs[i]['maxtfidfword'] = maxtfidfword\n",
    "  bar.update(i + 1)\n",
    "\n",
    "with open('data/tfidf.json', 'w') as json_file:\n",
    "  json_file.write(json.dumps(idfs, sort_keys=True, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 0:00:00] [00000000000000000000000000000000000] (ETA:  0:00:00) \r"
     ]
    }
   ],
   "source": [
    "# creating dictionary\n",
    "import json\n",
    "\n",
    "with open('data/tfidf.json') as json_file:\n",
    "  documents = json.load(json_file)\n",
    "\n",
    "bar = progress_bar().start()\n",
    "words = []\n",
    "\n",
    "for i, document in enumerate(documents):\n",
    "  bar.update(i + 1)\n",
    "  for word, val in document['frequency'].items():\n",
    "    if word not in words:\n",
    "      words.append(word)\n",
    "\n",
    "words_dict = dict(zip(range(len(words)), words))\n",
    "\n",
    "with open('data/dictionary.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(words_dict, sort_keys=True, indent=4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 0:00:01] [00000000000000000000000000000000000] (ETA:  0:00:00) \r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('data/dictionary.json') as json_file:\n",
    "  words = json.load(json_file)\n",
    "with open('data/tfidf.json') as json_file:\n",
    "  documents = json.load(json_file)\n",
    "\n",
    "# define function to get key from dictionary\n",
    "def get_key(val):\n",
    "  for key, value in words.items():\n",
    "    if val == value:\n",
    "      return key\n",
    "\n",
    "for i, document in enumerate(documents):\n",
    "  bar.update(i + 1)\n",
    "  result = {}\n",
    "  for word, val in document['frequency'].items():\n",
    "      if word in words.values():\n",
    "        result[get_key(word)] = val\n",
    "  documents[i]['frequency'] = result\n",
    "\n",
    "with open('data/dictionarized.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(documents, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 0:00:00] [00000000000000000000000000000000000] (ETA:  0:00:00) \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Clusters\n"
     ]
    }
   ],
   "source": [
    "# hierarchical clustering\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.cluster.vq as scv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pandas import DataFrame\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "N_CLUSTERS = 10\n",
    "\n",
    "with open('data/dictionary.json') as json_file:\n",
    "  dictionary = json.load(json_file)\n",
    "\n",
    "with open('data/dictionarized.json') as json_file:\n",
    "  documents = json.load(json_file)\n",
    "\n",
    "bar = progress_bar().start()\n",
    "\n",
    "print('Calculating Clusters')\n",
    "array = []\n",
    "for i, document in enumerate(documents):\n",
    "\tbar.update(i + 1)\n",
    "\tnew = []\n",
    "\tfor key in dictionary.keys():\n",
    "\t\tif not document['frequency'].get(key) is None:\n",
    "\t\t\tnew.append(document['frequency'].get(key))\n",
    "\t\telse:\n",
    "\t\t\tnew.append(0)\n",
    "\tarray.append(new)\n",
    "\n",
    "X = np.array(array)\n",
    "\n",
    "titles = []\n",
    "\n",
    "for document in documents:\n",
    "\ttitles.append(document['title'])\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=N_CLUSTERS, affinity='euclidean', linkage='ward')\n",
    "cluster = clustering.fit_predict(X)\n",
    "\n",
    "# for i in range(len(cluster)):\n",
    "# \tprint(f'Cluster: {cluster[i]}, Title: {titles[i]} \\n')\n",
    "\n",
    "# fig, dendogram = plt.subplots(figsize=(30,40))\n",
    "\n",
    "# dendogram = sch.dendrogram(sch.linkage(array, method='ward'), orientation='top')\n",
    "# plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plt.ylabel('Article')\n",
    "# plt.xlabel('Euclidean Distance')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('t'+str(4)+'sw'\n",
    "#             +'dendogram.png', dpi=200)\n",
    "\n",
    "dataframe = DataFrame({\n",
    "\t\t'Cluster': cluster,\n",
    "\t\t'Title': titles,\n",
    "})\n",
    "\n",
    "sorted_data_frame = dataframe.sort_values(by=['Cluster'])\n",
    "sorted_data_frame.to_csv('hierarchical_clustering.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95334295d287576637839bb6c89b1e9ed45a51d991237be8e658a0ed30ccbece"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
