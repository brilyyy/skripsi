{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar\n",
    "\n",
    "\n",
    "def progress_bar():\n",
    "    return progressbar.ProgressBar(maxval=78, widgets=[\n",
    "        ' [', progressbar.Timer(), '] ',\n",
    "        progressbar.Bar(marker='0', left='[', right=']'),\n",
    "        ' (', progressbar.ETA(), ') ',\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 0:00:00] [00000000000000000000000000000000000] (Time: 0:00:00) \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case folding done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# case folding\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import concurrent.futures\n",
    "\n",
    "with open('data/documents.json') as abstracts_json:\n",
    "  abstracts = json.load(abstracts_json)\n",
    "\n",
    "bar = progress_bar()\n",
    "bar.start()\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  answer = re.sub('[^a-z]+', ' ', abstract['abstract'].casefold())\n",
    "  abstracts[i]['abstract'] = answer\n",
    "\n",
    "with open('data/case_folded.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n",
    "  \n",
    "bar.finish()\n",
    "print('Case folding done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 0:00:01] [00000000000000000000000000000000000] (Time: 0:00:01) \n"
     ]
    }
   ],
   "source": [
    "# translate if english\n",
    "import json\n",
    "from langdetect import detect\n",
    "import translators as ts\n",
    "\n",
    "with open('data/case_folded.json') as case_folded_json:\n",
    "  abstracts = json.load(case_folded_json)\n",
    "\n",
    "\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  detector = detect(abstract['abstract'])\n",
    "  if detector == 'en':\n",
    "    translation = ts.google(abstract['abstract'], from_language='en', to_language='id')\n",
    "    abstracts[i]['abstract'] = translation\n",
    "\n",
    "\n",
    "with open('data/translated.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 0:00:00] [00000000000000000000000000000000000] (Time: 0:00:00) \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case folding done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# case folding again\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "processed = []\n",
    "count = 0\n",
    "with open('data/translated.json') as abstracts_json:\n",
    "  abstracts = json.load(abstracts_json)\n",
    "\n",
    "\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  answer = re.sub('[^a-z]+', ' ', abstract['abstract'].casefold())\n",
    "  abstracts[i]['abstract'] = answer\n",
    "\n",
    "with open('data/case_folded.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n",
    "\n",
    "bar.finish()\n",
    "print('Case folding done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 0:01:59] [00000000000000000000000000000000000] (Time: 0:01:59) \n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "import json\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "with open('data/case_folded.json') as case_folded_json:\n",
    "  abstracts = json.load(case_folded_json)\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "\n",
    "bar = progress_bar()\n",
    "bar.start()\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  stemmed = stemmer.stem(abstract['abstract'])\n",
    "  abstracts[i]['abstract'] = stemmed\n",
    "\n",
    "\n",
    "with open('data/stemmed_abstracts.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n",
    "\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 0:00:00] [                                  ] (ETA:  --:--:--) \r"
     ]
    }
   ],
   "source": [
    "# filtering\n",
    "import json\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "\n",
    "with open('data/stemmed_abstracts.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "additional_stopwords = []\n",
    "\n",
    "stopwords = StopWordRemoverFactory().get_stop_words() + additional_stopwords\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  words = abstract['abstract'].split()\n",
    "  filtered = ''\n",
    "  for word in words:\n",
    "    if word not in stopwords:\n",
    "      filtered += word + ' '\n",
    "  abstracts[i]['abstract'] = filtered\n",
    "\n",
    "with open('data/filtered.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/brilyyy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      " [Elapsed Time: 0:00:00] [                                  ] (ETA:  --:--:--) \r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "with open('data/filtered.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  frequency = nltk.FreqDist(nltk.word_tokenize(abstract['abstract']))\n",
    "  abstracts[i]['frequency'] = frequency\n",
    "\n",
    "with open('data/tokenized.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term frequency\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "with open('data/tokenized.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  result = {}\n",
    "  bagOfWordsCount = len(abstract['abstract'].split())\n",
    "  for word, count in abstract['frequency'].items():\n",
    "    if(count / float(bagOfWordsCount)):\n",
    "      result[word] = count / float(bagOfWordsCount)\n",
    "  abstracts[i]['frequency'] = result\n",
    "\n",
    "with open('data/termfrequency.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse document frequency\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "\n",
    "threshold = 4\n",
    "\n",
    "with open('data/tokenized.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "abstract_length = len(abstracts)\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  idfDict = dict.fromkeys(abstract['frequency'].keys(), 0)\n",
    "  result = {}\n",
    "  for key in idfDict.keys():\n",
    "    for abstract in abstracts:\n",
    "      if key in abstract['frequency']:\n",
    "        idfDict[key] += 1\n",
    "  \n",
    "  for word, val in idfDict.items():\n",
    "    if val > threshold:\n",
    "      result[word] = math.log10(abstract_length / float(val) + 1)\n",
    "  abstracts[i]['frequency'] = result\n",
    "\n",
    "with open('data/idf.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "import json\n",
    "\n",
    "with open('data/idf.json') as idf_json:\n",
    "  idfs = json.load(idf_json)\n",
    "\n",
    "with open('data/termfrequency.json') as tf_json:\n",
    "  tfs = json.load(tf_json)\n",
    "\n",
    "\n",
    "\n",
    "for i, idf in enumerate(idfs):\n",
    "  tfidf = {}\n",
    "  maxtfidf = 0\n",
    "  for word, val in idf['frequency'].items():\n",
    "    if tfs[i]['frequency'].get(word) is not None:\n",
    "      tfidf[word] = val*float(tfs[i]['frequency'].get(word))\n",
    "      if maxtfidf < tfidf[word]:\n",
    "        maxtfidf = tfidf[word]\n",
    "        maxtfidfword = word\n",
    "  idfs[i]['frequency'] = tfidf\n",
    "  idfs[i]['maxtfidfword'] = maxtfidfword\n",
    "\n",
    "with open('data/tfidf.json', 'w') as json_file:\n",
    "  json_file.write(json.dumps(idfs, sort_keys=True, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary\n",
    "import json\n",
    "\n",
    "with open('data/tfidf.json') as json_file:\n",
    "  documents = json.load(json_file)\n",
    "\n",
    "\n",
    "words = []\n",
    "\n",
    "for i, document in enumerate(documents):\n",
    "  for word, val in document['frequency'].items():\n",
    "    if word not in words:\n",
    "      words.append(word)\n",
    "\n",
    "words_dict = dict(zip(range(len(words)), words))\n",
    "\n",
    "with open('data/dictionary.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(words_dict, sort_keys=True, indent=4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace word with dictionary id\n",
    "\n",
    "import json\n",
    "\n",
    "with open('data/dictionary.json') as json_file:\n",
    "  words = json.load(json_file)\n",
    "with open('data/tfidf.json') as json_file:\n",
    "  documents = json.load(json_file)\n",
    "\n",
    "# define function to get key from dictionary\n",
    "def get_key(val):\n",
    "  for key, value in words.items():\n",
    "    if val == value:\n",
    "      return key\n",
    "\n",
    "for i, document in enumerate(documents):\n",
    "  result = {}\n",
    "  for word, val in document['frequency'].items():\n",
    "      if word in words.values():\n",
    "        result[get_key(word)] = val\n",
    "  documents[i]['frequency'] = result\n",
    "\n",
    "with open('data/dictionarized.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(documents, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Clusters\n"
     ]
    }
   ],
   "source": [
    "# hierarchical clustering\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.cluster.vq as scv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pandas import DataFrame\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "N_CLUSTERS = 10\n",
    "\n",
    "with open('data/dictionary.json') as json_file:\n",
    "  dictionary = json.load(json_file)\n",
    "\n",
    "with open('data/dictionarized.json') as json_file:\n",
    "  documents = json.load(json_file)\n",
    "\n",
    "# \n",
    "\n",
    "print('Calculating Clusters')\n",
    "array = []\n",
    "for i, document in enumerate(documents):\n",
    "\tnew = []\n",
    "\tfor key in dictionary.keys():\n",
    "\t\tif not document['frequency'].get(key) is None:\n",
    "\t\t\tnew.append(document['frequency'].get(key))\n",
    "\t\telse:\n",
    "\t\t\tnew.append(0)\n",
    "\tarray.append(new)\n",
    "\n",
    "X = np.array(array)\n",
    "\n",
    "titles = []\n",
    "\n",
    "for document in documents:\n",
    "\ttitles.append(document['title'])\n",
    "\n",
    "hierachical = AgglomerativeClustering(n_clusters=N_CLUSTERS, affinity='euclidean', linkage='ward')\n",
    "cluster = hierachical.fit_predict(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0).fit(X)\n",
    "\n",
    "\n",
    "# for i in range(len(cluster)):\n",
    "# \tprint(f'Cluster: {cluster[i]}, Title: {titles[i]} \\n')\n",
    "\n",
    "# fig, dendogram = plt.subplots(figsize=(30,40))\n",
    "\n",
    "# dendogram = sch.dendrogram(sch.linkage(array, method='ward'), orientation='top')\n",
    "# plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plt.ylabel('Article')\n",
    "# plt.xlabel('Euclidean Distance')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('t'+str(4)+'sw'\n",
    "#             +'dendogram.png', dpi=200)\n",
    "\n",
    "dataframe = DataFrame({\n",
    "\t\t'Cluster': cluster,\n",
    "\t\t'Title': titles,\n",
    "})\n",
    "\n",
    "kmeans_dataframe = DataFrame({\n",
    "\t\t'Cluster': kmeans.labels_.tolist(),\n",
    "\t\t'Title': titles,\n",
    "})\n",
    "\n",
    "sorted_data_frame = dataframe.sort_values(by=['Cluster'])\n",
    "sorted_data_frame.to_excel('hierarchical_clustering.xlsx', index=False)\n",
    "sorted_data_frame = kmeans_dataframe.sort_values(by=['Cluster'])\n",
    "sorted_data_frame.to_excel('kmeans_clustering.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95334295d287576637839bb6c89b1e9ed45a51d991237be8e658a0ed30ccbece"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
