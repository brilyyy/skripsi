{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case folding done\n"
     ]
    }
   ],
   "source": [
    "# case folding\n",
    "import json\n",
    "import re\n",
    "\n",
    "with open('data/documents-id.json') as abstracts_json:\n",
    "  abstracts = json.load(abstracts_json)\n",
    "\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  answer = re.sub('[^a-z]+', ' ', abstract['abstrak'].casefold())\n",
    "  abstracts[i]['abstrak'] = answer\n",
    "\n",
    "with open('data/case_folded.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n",
    "        \n",
    "print('Case folding done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 19\u001b[0m\n\u001b[1;32m     12\u001b[0m stemmer \u001b[39m=\u001b[39m factory\u001b[39m.\u001b[39mcreate_stemmer()\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m i, abstract \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(abstracts):\n\u001b[1;32m     18\u001b[0m   \u001b[39m# stemmed = stemmer.stem_kalimat(abstract['abstrak'])\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m   stemmed \u001b[39m=\u001b[39m stemmer\u001b[39m.\u001b[39;49mstem(abstract[\u001b[39m'\u001b[39;49m\u001b[39mabstrak\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     20\u001b[0m   abstracts[i][\u001b[39m'\u001b[39m\u001b[39mabstrak\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m stemmed\n\u001b[1;32m     23\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdata/stemmed_abstracts.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m outfile:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/venv311/lib/python3.11/site-packages/Sastrawi/Stemmer/CachedStemmer.py:20\u001b[0m, in \u001b[0;36mCachedStemmer.stem\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     18\u001b[0m     stems\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache\u001b[39m.\u001b[39mget(word))\n\u001b[1;32m     19\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     stem \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdelegatedStemmer\u001b[39m.\u001b[39;49mstem(word)\n\u001b[1;32m     21\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache\u001b[39m.\u001b[39mset(word, stem)\n\u001b[1;32m     22\u001b[0m     stems\u001b[39m.\u001b[39mappend(stem)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/venv311/lib/python3.11/site-packages/Sastrawi/Stemmer/Stemmer.py:27\u001b[0m, in \u001b[0;36mStemmer.stem\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m stems \u001b[39m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[0;32m---> 27\u001b[0m     stems\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstem_word(word))\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(stems)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/venv311/lib/python3.11/site-packages/Sastrawi/Stemmer/Stemmer.py:36\u001b[0m, in \u001b[0;36mStemmer.stem_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstem_plural_word(word)\n\u001b[1;32m     35\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstem_singular_word(word)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/venv311/lib/python3.11/site-packages/Sastrawi/Stemmer/Stemmer.py:84\u001b[0m, in \u001b[0;36mStemmer.stem_singular_word\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Stem a singular word to its common stem form.\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m context \u001b[39m=\u001b[39m Context(word, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdictionary, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisitor_provider)\n\u001b[0;32m---> 84\u001b[0m context\u001b[39m.\u001b[39;49mexecute()\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m context\u001b[39m.\u001b[39mresult\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/venv311/lib/python3.11/site-packages/Sastrawi/Stemmer/Context/Context.py:37\u001b[0m, in \u001b[0;36mContext.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Execute stemming process; the result can be retrieved with result\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m#step 1 - 5\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_stemming_process()\n\u001b[1;32m     39\u001b[0m \u001b[39m#step 6\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdictionary\u001b[39m.\u001b[39mcontains(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_word):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/venv311/lib/python3.11/site-packages/Sastrawi/Stemmer/Context/Context.py:80\u001b[0m, in \u001b[0;36mContext.start_stemming_process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[39m#step 4, 5\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mremove_prefixes()\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdictionary\u001b[39m.\u001b[39mcontains(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_word):\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/venv311/lib/python3.11/site-packages/Sastrawi/Stemmer/Context/Context.py:89\u001b[0m, in \u001b[0;36mContext.remove_prefixes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremove_prefixes\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     88\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[0;32m---> 89\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccept_prefix_visitors(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprefix_pisitors)\n\u001b[1;32m     90\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdictionary\u001b[39m.\u001b[39mcontains(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_word):\n\u001b[1;32m     91\u001b[0m             \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/venv311/lib/python3.11/site-packages/Sastrawi/Stemmer/Context/Context.py:110\u001b[0m, in \u001b[0;36mContext.accept_prefix_visitors\u001b[0;34m(self, visitors)\u001b[0m\n\u001b[1;32m    108\u001b[0m removalCount \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremovals)\n\u001b[1;32m    109\u001b[0m \u001b[39mfor\u001b[39;00m visitor \u001b[39min\u001b[39;00m visitors:\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccept(visitor)\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdictionary\u001b[39m.\u001b[39mcontains(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_word):\n\u001b[1;32m    112\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_word\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/venv311/lib/python3.11/site-packages/Sastrawi/Stemmer/Context/Context.py:97\u001b[0m, in \u001b[0;36mContext.accept\u001b[0;34m(self, visitor)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maccept\u001b[39m(\u001b[39mself\u001b[39m, visitor):\n\u001b[0;32m---> 97\u001b[0m     visitor\u001b[39m.\u001b[39;49mvisit(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.2/envs/venv311/lib/python3.11/site-packages/Sastrawi/Stemmer/Context/Visitor/AbstractDisambiguatePrefixRule.py:15\u001b[0m, in \u001b[0;36mAbstractDisambiguatePrefixRule.visit\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m disambiguator \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisambiguators:\n\u001b[1;32m     14\u001b[0m     result \u001b[39m=\u001b[39m disambiguator\u001b[39m.\u001b[39mdisambiguate(context\u001b[39m.\u001b[39mcurrent_word)\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mdictionary\u001b[39m.\u001b[39mcontains(result):\n\u001b[1;32m     16\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "import json\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from mpstemmer import MPStemmer\n",
    "\n",
    "# stemmer = MPStemmer()\n",
    "\n",
    "with open('data/case_folded.json') as case_folded_json:\n",
    "  abstracts = json.load(case_folded_json)\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  # stemmed = stemmer.stem_kalimat(abstract['abstrak'])\n",
    "  stemmed = stemmer.stem(abstract['abstrak'])\n",
    "  abstracts[i]['abstrak'] = stemmed\n",
    "\n",
    "\n",
    "with open('data/stemmed_abstracts.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering\n",
    "import json\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n",
    "\n",
    "with open('data/stemmed_abstracts.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "additional_stopwords = []\n",
    "\n",
    "stopwords = StopWordRemoverFactory().get_stop_words() + additional_stopwords\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  words = abstract['abstrak'].split()\n",
    "  filtered = ''\n",
    "  for word in words:\n",
    "    if word not in stopwords:\n",
    "      filtered += word + ' '\n",
    "  abstracts[i]['abstrak'] = filtered\n",
    "\n",
    "with open('data/filtered.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/brilyyy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "with open('data/filtered.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  frequency = nltk.FreqDist(nltk.word_tokenize(abstract['abstrak']))\n",
    "  abstracts[i]['frequency'] = frequency\n",
    "\n",
    "with open('data/tokenized.json', 'w') as outfile:\n",
    "  outfile.write(json.dumps(abstracts, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term frequency\n",
    "import json\n",
    "\n",
    "with open('data/tokenized.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  result = {}\n",
    "  bagOfWordsCount = len(abstract['abstrak'].split())\n",
    "  for word, count in abstract['frequency'].items():\n",
    "    if(count / float(bagOfWordsCount)):\n",
    "      result[word] = count / float(bagOfWordsCount)\n",
    "  abstracts[i]['frequency'] = result\n",
    "\n",
    "with open('data/termfrequency.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse document frequency\n",
    "import json\n",
    "import math\n",
    "\n",
    "\n",
    "threshold = 10\n",
    "\n",
    "with open('data/tokenized.json') as json_file:\n",
    "  abstracts = json.load(json_file)\n",
    "\n",
    "\n",
    "\n",
    "abstract_length = len(abstracts)\n",
    "\n",
    "for i, abstract in enumerate(abstracts):\n",
    "  idfDict = dict.fromkeys(abstract['frequency'].keys(), 0)\n",
    "  result = {}\n",
    "  for key in idfDict.keys():\n",
    "    for abstract in abstracts:\n",
    "      if key in abstract['frequency']:\n",
    "        idfDict[key] += 1\n",
    "  for word, val in idfDict.items():\n",
    "    if val > threshold:\n",
    "      result[word] = math.log10(abstract_length / float(val) + 1)\n",
    "  abstracts[i]['frequency'] = result\n",
    "\n",
    "with open('data/idf.json', 'w') as outfile:\n",
    "\toutfile.write(json.dumps(abstracts, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf\n",
    "import json\n",
    "\n",
    "with open('data/idf.json') as idf_json:\n",
    "  idfs = json.load(idf_json)\n",
    "\n",
    "with open('data/termfrequency.json') as tf_json:\n",
    "  tfs = json.load(tf_json)\n",
    "\n",
    "\n",
    "\n",
    "for i, idf in enumerate(idfs):\n",
    "  tfidf = {}\n",
    "  maxtfidf = 0\n",
    "  for word, val in idf['frequency'].items():\n",
    "    if tfs[i]['frequency'].get(word) is not None:\n",
    "      tfidf[word] = val*float(tfs[i]['frequency'].get(word))\n",
    "      if maxtfidf < tfidf[word]:\n",
    "        maxtfidf = tfidf[word]\n",
    "        maxtfidfword = word\n",
    "  idfs[i]['frequency'] = tfidf\n",
    "  idfs[i]['maxtfidfword'] = maxtfidfword\n",
    "\n",
    "with open('data/tfidf.json', 'w') as json_file:\n",
    "  json_file.write(json.dumps(idfs, sort_keys=True, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary\n",
    "import json\n",
    "\n",
    "with open('data/tfidf.json') as json_file:\n",
    "  documents = json.load(json_file)\n",
    "\n",
    "\n",
    "words = []\n",
    "\n",
    "for i, document in enumerate(documents):\n",
    "  for word, val in document['frequency'].items():\n",
    "    if word not in words:\n",
    "      words.append(word)\n",
    "\n",
    "words_dict = dict(zip(range(len(words)), words))\n",
    "\n",
    "with open('data/dictionary.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(words_dict, sort_keys=True, indent=4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace word with dictionary id\n",
    "\n",
    "import json\n",
    "\n",
    "with open('data/dictionary.json') as json_file:\n",
    "  words = json.load(json_file)\n",
    "with open('data/tfidf.json') as json_file:\n",
    "  documents = json.load(json_file)\n",
    "\n",
    "# define function to get key from dictionary\n",
    "def get_key(val):\n",
    "  for key, value in words.items():\n",
    "    if val == value:\n",
    "      return key\n",
    "\n",
    "for i, document in enumerate(documents):\n",
    "  result = {}\n",
    "  for word, val in document['frequency'].items():\n",
    "      if word in words.values():\n",
    "        result[get_key(word)] = val\n",
    "  documents[i]['frequency'] = result\n",
    "\n",
    "with open('data/dictionarized.json','w') as outfile:\n",
    "\toutfile.write(json.dumps(documents, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Clusters\n"
     ]
    }
   ],
   "source": [
    "# hierarchical clustering\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.cluster.vq as scv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pandas import DataFrame\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "N_CLUSTERS = 23\n",
    "\n",
    "with open('data/dictionary.json') as json_file:\n",
    "    dictionary = json.load(json_file)\n",
    "\n",
    "with open('data/dictionarized.json') as json_file:\n",
    "    documents = json.load(json_file)\n",
    "\n",
    "#\n",
    "\n",
    "print('Calculating Clusters')\n",
    "array = []\n",
    "for i, document in enumerate(documents):\n",
    "    new = []\n",
    "    for key in dictionary.keys():\n",
    "        if not document['frequency'].get(key) is None:\n",
    "            new.append(document['frequency'].get(key))\n",
    "        else:\n",
    "            new.append(0)\n",
    "    array.append(new)\n",
    "\n",
    "X = np.array(array)\n",
    "\n",
    "titles = []\n",
    "\n",
    "for document in documents:\n",
    "    titles.append(document['judul'])\n",
    "\n",
    "hierachical = AgglomerativeClustering(n_clusters=N_CLUSTERS,\n",
    "                                      linkage='ward').fit_predict(X)\n",
    "kmedoids = KMedoids(n_clusters=N_CLUSTERS, max_iter=1000,).fit(X)\n",
    "\n",
    "\n",
    "dataframe = DataFrame({\n",
    "  'Cluster': hierachical,\n",
    "  'Title': titles,\n",
    "})\n",
    "\n",
    "kmeds_dataframe = DataFrame({\n",
    "  'Cluster': kmedoids.labels_.tolist(),\n",
    "  'Title': titles,\n",
    "})\n",
    "\n",
    "sorted_data_frame = dataframe.sort_values(by=['Cluster'])\n",
    "sorted_data_frame.to_excel('hierarchical_clustering.xlsx', index=False)\n",
    "# sorted_data_frame = kmeans_dataframe.sort_values(by=['Cluster'])\n",
    "# sorted_data_frame.to_excel('kmeans_clustering.xlsx', index=False)\n",
    "sorted_data_frame = kmeds_dataframe.sort_values(by=['Cluster'])\n",
    "sorted_data_frame.to_excel('kmeds_clustering.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82d74610fba66d22e01faa28a65a289a119f27a0b2a61fc6029b1d06947e4a01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
